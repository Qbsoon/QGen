{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7244383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "from torchvision import transforms\n",
    "from imagen_pytorch.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from download_dataset import download_dataset\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import json\n",
    "from transformers import T5Tokenizer, T5EncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of ImageDataset class, used later\n",
    "\n",
    "image_size = 256 # Target size for the second U-Net\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),       # Resize smaller edge to image_size\n",
    "    transforms.CenterCrop(image_size),   # Crop center to image_size x image_size\n",
    "    transforms.ToTensor(),               # Convert PIL image [0, 255] to tensor [0, 1]\n",
    "    # Optional: Add normalization if required by the model, e.g., for [-1, 1] range\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, text_embeds, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.files = [os.path.join(root_dir, file) for file in os.listdir(root_dir) if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.text_embeds = text_embeds\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.files) == len(self.text_embeds), \"Number of images is different from number of text encodings\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f'Error loading image {img_path}: {e}')\n",
    "            return None, None\n",
    "        \n",
    "        text_embedding = self.text_embeds[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image_t = self.transform(image)\n",
    "        return image_t, text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880491d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imagen & Unets\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1, unet2),\n",
    "    text_encoder_name = 't5-large',\n",
    "    image_sizes = (64, 256),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a2696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 632 images in datasets/dimensi0n/imagenet-256/zebra\n",
      "Processed batch 1/79, Captions generated: 8\n",
      "Processed batch 2/79, Captions generated: 16\n",
      "Processed batch 3/79, Captions generated: 24\n",
      "Processed batch 4/79, Captions generated: 32\n",
      "Processed batch 5/79, Captions generated: 40\n",
      "Processed batch 6/79, Captions generated: 48\n",
      "Processed batch 7/79, Captions generated: 56\n",
      "Processed batch 8/79, Captions generated: 64\n",
      "Processed batch 9/79, Captions generated: 72\n",
      "Processed batch 10/79, Captions generated: 80\n",
      "Processed batch 11/79, Captions generated: 88\n",
      "Processed batch 12/79, Captions generated: 96\n",
      "Processed batch 13/79, Captions generated: 104\n",
      "Processed batch 14/79, Captions generated: 112\n",
      "Processed batch 15/79, Captions generated: 120\n",
      "Processed batch 16/79, Captions generated: 128\n",
      "Processed batch 17/79, Captions generated: 136\n",
      "Processed batch 18/79, Captions generated: 144\n",
      "Processed batch 19/79, Captions generated: 152\n",
      "Processed batch 20/79, Captions generated: 160\n",
      "Processed batch 21/79, Captions generated: 168\n",
      "Processed batch 22/79, Captions generated: 176\n",
      "Processed batch 23/79, Captions generated: 184\n",
      "Processed batch 24/79, Captions generated: 192\n",
      "Processed batch 25/79, Captions generated: 200\n",
      "Processed batch 26/79, Captions generated: 208\n",
      "Processed batch 27/79, Captions generated: 216\n",
      "Processed batch 28/79, Captions generated: 224\n",
      "Processed batch 29/79, Captions generated: 232\n",
      "Processed batch 30/79, Captions generated: 240\n",
      "Processed batch 31/79, Captions generated: 248\n",
      "Processed batch 32/79, Captions generated: 256\n",
      "Processed batch 33/79, Captions generated: 264\n",
      "Processed batch 34/79, Captions generated: 272\n",
      "Processed batch 35/79, Captions generated: 280\n",
      "Processed batch 36/79, Captions generated: 288\n",
      "Processed batch 37/79, Captions generated: 296\n",
      "Processed batch 38/79, Captions generated: 304\n",
      "Processed batch 39/79, Captions generated: 312\n",
      "Processed batch 40/79, Captions generated: 320\n",
      "Processed batch 41/79, Captions generated: 328\n",
      "Processed batch 42/79, Captions generated: 336\n",
      "Processed batch 43/79, Captions generated: 344\n",
      "Processed batch 44/79, Captions generated: 352\n",
      "Processed batch 45/79, Captions generated: 360\n",
      "Processed batch 46/79, Captions generated: 368\n",
      "Processed batch 47/79, Captions generated: 376\n",
      "Processed batch 48/79, Captions generated: 384\n",
      "Processed batch 49/79, Captions generated: 392\n",
      "Processed batch 50/79, Captions generated: 400\n",
      "Processed batch 51/79, Captions generated: 408\n",
      "Processed batch 52/79, Captions generated: 416\n",
      "Processed batch 53/79, Captions generated: 424\n",
      "Processed batch 54/79, Captions generated: 432\n",
      "Processed batch 55/79, Captions generated: 440\n",
      "Processed batch 56/79, Captions generated: 448\n",
      "Processed batch 57/79, Captions generated: 456\n",
      "Processed batch 58/79, Captions generated: 464\n",
      "Processed batch 59/79, Captions generated: 472\n",
      "Processed batch 60/79, Captions generated: 480\n",
      "Processed batch 61/79, Captions generated: 488\n",
      "Processed batch 62/79, Captions generated: 496\n",
      "Processed batch 63/79, Captions generated: 504\n",
      "Processed batch 64/79, Captions generated: 512\n",
      "Processed batch 65/79, Captions generated: 520\n",
      "Processed batch 66/79, Captions generated: 528\n",
      "Processed batch 67/79, Captions generated: 536\n",
      "Processed batch 68/79, Captions generated: 544\n",
      "Processed batch 69/79, Captions generated: 552\n",
      "Processed batch 70/79, Captions generated: 560\n",
      "Processed batch 71/79, Captions generated: 568\n",
      "Processed batch 72/79, Captions generated: 576\n",
      "Processed batch 73/79, Captions generated: 584\n",
      "Processed batch 74/79, Captions generated: 592\n",
      "Processed batch 75/79, Captions generated: 600\n",
      "Processed batch 76/79, Captions generated: 608\n",
      "Processed batch 77/79, Captions generated: 616\n",
      "Processed batch 78/79, Captions generated: 624\n",
      "Processed batch 79/79, Captions generated: 632\n",
      "Captions saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Code for generating the image captions\n",
    "\n",
    "source_directory = 'datasets/dimensi0n/imagenet-256/zebra'\n",
    "captions_creating_batch_size = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n",
    "# --- List Image Files ---\n",
    "# Ensure consistent order by sorting\n",
    "image_files = sorted([\n",
    "    os.path.join(source_directory, f)\n",
    "    for f in os.listdir(source_directory)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "print(f\"Found {len(image_files)} images in {source_directory}\")\n",
    "\n",
    "# --- Generate Captions ---\n",
    "captions_data = [] # List to store captions\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for inference\n",
    "    for i in range(0, len(image_files), captions_creating_batch_size):\n",
    "        batch_files = image_files[i:i+captions_creating_batch_size]\n",
    "        raw_images = []\n",
    "        valid_files_in_batch = [] # Keep track of files successfully loaded in this batch\n",
    "\n",
    "        # Load images in the batch\n",
    "        for img_path in batch_files:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                raw_images.append(image)\n",
    "                valid_files_in_batch.append(img_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not load image {img_path}. Skipping. Error: {e}\")\n",
    "\n",
    "        if not raw_images: # Skip if no images were loaded in the batch\n",
    "            print(f\"Skipping empty batch starting at index {i}\")\n",
    "            continue\n",
    "\n",
    "        # Preprocess images\n",
    "        inputs = processor(images=raw_images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate captions\n",
    "        outputs = model.generate(**inputs, max_length=75, num_beams=4)\n",
    "\n",
    "        # Decode captions\n",
    "        decoded_captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Store results for this batch\n",
    "        for img_path, caption in zip(valid_files_in_batch, decoded_captions):\n",
    "            captions_data.append({'image_path': img_path, 'caption': caption.strip()})\n",
    "\n",
    "        print(f\"Processed batch {i//captions_creating_batch_size + 1}/{(len(image_files) + captions_creating_batch_size - 1)//captions_creating_batch_size}, Captions generated: {len(captions_data)}\")\n",
    "\n",
    "\n",
    "# --- Save Captions ---\n",
    "try:\n",
    "    with open(os.path.join(source_directory, 'captions.json'), 'w') as f:\n",
    "        json.dump(captions_data, f, indent=4)\n",
    "    print(\"Captions saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving captions: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af6865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5 Tokenizer and Encoder Model (t5-large)...\n",
      "T5 models loaded.\n",
      "Generated text embeddings shape: torch.Size([632, 256, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Code for encoding the image captions\n",
    "\n",
    "source_directory = 'datasets/dimensi0n/imagenet-256/zebra'\n",
    "with open(os.path.join(source_directory, 'captions.json'), 'r') as f:\n",
    "    captions_data = json.load(f)\n",
    "\n",
    "image_files = sorted([\n",
    "    os.path.join(source_directory, f)\n",
    "    for f in os.listdir(source_directory)\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "\n",
    "captions = [image['caption'] for image in captions_data if image['image_path'] in image_files]\n",
    "\n",
    "t5_model_name = 't5-large'\n",
    "print(f\"Loading T5 Tokenizer and Encoder Model ({t5_model_name})...\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "t5_model = T5EncoderModel.from_pretrained(t5_model_name).cuda() # Move model to GPU\n",
    "t5_model.eval() # Set T5 model to evaluation mode\n",
    "print(\"T5 models loaded.\")\n",
    "\n",
    "text_encoding_batch_size = 16\n",
    "max_text_seq_len = 256 \n",
    "all_text_embeds = []\n",
    "\n",
    "with torch.no_grad(): # No need to track gradients for encoding\n",
    "    for i in range(0, len(captions), text_encoding_batch_size):\n",
    "        batch_texts = captions[i:i+text_encoding_batch_size]\n",
    "        tokenized_inputs = t5_tokenizer(\n",
    "            batch_texts,\n",
    "            padding='max_length',       # Pad to max_length\n",
    "            truncation=True,            # Truncate sequences longer than max_length\n",
    "            max_length=max_text_seq_len,# The maximum sequence length\n",
    "            return_tensors='pt'         # Return PyTorch tensors\n",
    "        ).to(imagen.device) # Move tokenized inputs to the same device as the T5 model\n",
    "\n",
    "        # Get embeddings from T5 model\n",
    "        # T5EncoderModel output is a BaseModelOutput, embeddings are in last_hidden_state\n",
    "        outputs = t5_model(input_ids=tokenized_inputs.input_ids, attention_mask=tokenized_inputs.attention_mask)\n",
    "        text_embeds_batch = outputs.last_hidden_state # Shape: [batch_size, max_text_seq_len, 1024]\n",
    "\n",
    "        # Append to list (move to CPU to save GPU memory during accumulation)\n",
    "        all_text_embeds.append(text_embeds_batch.cpu())\n",
    "\n",
    "# Concatenate all embedding batches\n",
    "text_embeddings_tensor = torch.cat(all_text_embeds, dim=0)\n",
    "print(f\"Generated text embeddings shape: {text_embeddings_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dataset\n",
    "\n",
    "dataset = ImageDataset(root_dir=source_directory, text_embeds=text_embeddings_tensor, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3cbd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Handles None returns from failed image loads\n",
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x[0] is not None, batch)) # Filter out samples where image loading failed\n",
    "    if not batch: return torch.Tensor(), torch.Tensor() # Return empty tensors if batch is empty\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn) # num_workers sets parallel data loading threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bfa674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for training the model\n",
    "\n",
    "imagen.train()\n",
    "trainer = ImagenTrainer(imagen).cuda()  # Trainer is left, so I don't forget about it\n",
    "trainer.train()                         # Generally it should replace the code below, because it wraps its own training code instead of writing the training manually, but I couldn't get it to work properly\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(imagen.parameters(), lr=1e-4) # Example optimizer, not sure if correct one\n",
    "\n",
    "#Training in epochs, each unet is trained once in each epoch\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images_batch, text_embeds_batch in dataloader:\n",
    "        if images_batch.numel() == 0: continue # Skip if batch is empty after filtering\n",
    "\n",
    "        images_batch = images_batch.cuda()\n",
    "        text_embeds_batch = text_embeds_batch.cuda()\n",
    "\n",
    "        # feed images into imagen, training each unet in the cascade\n",
    "        for i in (1, 2): # Train both U-Nets\n",
    "             # You might need different image sizes/transforms per U-Net\n",
    "             # For simplicity, using the same batch here. Consider resizing/downsampling\n",
    "             # images_for_unet = F.interpolate(images_batch, size=imagen.image_sizes[i-1]) if i==1 else images_batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = imagen(images_batch, text_embeds = text_embeds_batch, unet_number = i)\n",
    "            print(f\"  Unet {i}, Loss: {loss.item()}\") # Optional: print loss\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f95434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for generating images from a ready model\n",
    "\n",
    "images = imagen.sample(texts = [\n",
    "    'a whale breaching from afar',\n",
    "    'young girl blowing out candles on her birthday cake',\n",
    "    'fireworks with blue and green sparkles'\n",
    "], cond_scale = 3.)\n",
    "\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for displaying the generated images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# --- Display the generated images ---\n",
    "# Move images tensor to CPU and detach from gradient computation\n",
    "images_cpu = images.cpu().detach()\n",
    "\n",
    "# If your images are normalized to [-1, 1], denormalize them to [0, 1]\n",
    "# images_cpu = (images_cpu + 1) / 2 # Uncomment if you used transforms.Normalize(...)\n",
    "\n",
    "# Clamp values to [0, 1] just in case\n",
    "images_cpu = torch.clamp(images_cpu, 0, 1)\n",
    "\n",
    "# Make a grid of images (rows = number of samples)\n",
    "grid = vutils.make_grid(images_cpu, nrow=len(images_cpu), padding=2, normalize=False) # normalize=False as we clamped\n",
    "\n",
    "# Convert grid tensor to numpy array and transpose dimensions for matplotlib\n",
    "# (C, H, W) -> (H, W, C)\n",
    "np_grid = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Display the grid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "# --- End Display ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
