{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a1c17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-18 10:16:51.874547: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-18 10:16:52.008501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744964212.062044     415 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744964212.076451     415 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-18 10:16:52.225345: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:datasets:PyTorch version 2.6.0 available.\n",
      "INFO:datasets:TensorFlow version 2.18.0 available.\n"
     ]
    }
   ],
   "source": [
    "from backend import workflow\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "import torch\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820e18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5 Tokenizer and Encoder Model (t5-large)...\n",
      "T5 models loaded.\n"
     ]
    }
   ],
   "source": [
    "source_directory = 'datasets/dimensi0n/imagenet-256/zebra'\n",
    "using_checkpoint = 'checkpoints/a_model.pt'\n",
    "\n",
    "dataloader = workflow(source_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc19cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n"
     ]
    }
   ],
   "source": [
    "# Imagen & Unets\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1, unet2),\n",
    "    text_encoder_name = 't5-large',\n",
    "    image_sizes = (64, 256),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a86313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing imagen and trainer\n",
    "\n",
    "imagen.train()\n",
    "trainer = ImagenTrainer(imagen).cuda()\n",
    "trainer.train()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "439ad52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "\n",
    "trainer.load(using_checkpoint)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91808d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for running the training\n",
    "\n",
    "num_epochs = 10\n",
    "unet_toTrain = 2    # Can't train both unets at the same time, need to save checkpoint and re-initiate the trainer when changing unets  z\n",
    "\n",
    "#Training in epochs, each unet is trained once in each epoch\n",
    "losses = 0\n",
    "for epoch in range(num_epochs):\n",
    "    clear_output()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training UNet {unet_toTrain}, Prev epoch avg loss: {losses/len(dataloader)}\")\n",
    "    losses = 0\n",
    "    for images_batch, text_embeds_batch in dataloader:\n",
    "        if images_batch.numel() == 0: continue # Skip if batch is empty after filtering\n",
    "\n",
    "        images_batch = images_batch.cuda()\n",
    "        text_embeds_batch = text_embeds_batch.cuda()\n",
    "\n",
    "        loss = trainer(images_batch, text_embeds = text_embeds_batch, unet_number = unet_toTrain)\n",
    "        losses += loss\n",
    "        trainer.update(unet_number = unet_toTrain)\n",
    "        print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c5936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved to checkpoints/a_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer.save(using_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5baa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/120, Training UNet 2, Prev epoch avg loss: 0.020036027427833482\n",
      "Loss: 0.04046138748526573\n",
      "Loss: 0.029270917177200317\n",
      "Loss: 0.041758596897125244\n",
      "Loss: 0.013828713446855545\n",
      "Loss: 0.01839420758187771\n",
      "Loss: 0.010121184401214123\n",
      "Loss: 0.014673742465674877\n",
      "Loss: 0.006778263952583075\n",
      "Loss: 0.014876039698719978\n",
      "Loss: 0.02069457247853279\n",
      "Loss: 0.00493717473000288\n",
      "Loss: 0.021929875016212463\n",
      "Loss: 0.02909013256430626\n",
      "Loss: 0.011233475059270859\n",
      "Loss: 0.023960234597325325\n",
      "Loss: 0.02447344921529293\n",
      "Loss: 0.013372305780649185\n",
      "Loss: 0.005941904615610838\n",
      "Loss: 0.0025979834608733654\n",
      "Loss: 0.016660477966070175\n",
      "Loss: 0.009339967742562294\n",
      "Loss: 0.01728961616754532\n",
      "Loss: 0.01661369390785694\n",
      "Loss: 0.01172249112278223\n",
      "Loss: 0.026525042951107025\n",
      "Loss: 0.02476748265326023\n",
      "Loss: 0.037353239953517914\n",
      "Loss: 0.02223319560289383\n",
      "Loss: 0.009747845120728016\n",
      "Loss: 0.014803659170866013\n",
      "Loss: 0.017430288717150688\n",
      "Loss: 0.014379195868968964\n",
      "Loss: 0.029190320521593094\n",
      "Loss: 0.014646803960204124\n",
      "Loss: 0.028479348868131638\n",
      "Loss: 0.02483202889561653\n",
      "Loss: 0.009974940679967403\n",
      "Loss: 0.026938972994685173\n",
      "Loss: 0.009087500162422657\n",
      "Loss: 0.03254716843366623\n",
      "Loss: 0.036502763628959656\n",
      "Loss: 0.022859923541545868\n",
      "Loss: 0.0031683510169386864\n",
      "Loss: 0.07193756103515625\n",
      "Loss: 0.0155506432056427\n",
      "Loss: 0.03176902234554291\n",
      "Loss: 0.01720794476568699\n",
      "Loss: 0.01414228044450283\n",
      "Loss: 0.008565934374928474\n",
      "Loss: 0.023814057931303978\n",
      "Loss: 0.013149337843060493\n",
      "Loss: 0.017688145861029625\n",
      "Loss: 0.009117507375776768\n",
      "Loss: 0.011644480749964714\n",
      "Loss: 0.028372034430503845\n",
      "Loss: 0.020493941381573677\n",
      "Loss: 0.02093375101685524\n",
      "Loss: 0.04536818340420723\n",
      "Loss: 0.028369542211294174\n",
      "Loss: 0.01204498577862978\n",
      "Loss: 0.013132117688655853\n",
      "Loss: 0.024320155382156372\n",
      "Loss: 0.017297375947237015\n",
      "Loss: 0.01568755879998207\n",
      "Loss: 0.03662048280239105\n",
      "Loss: 0.017820850014686584\n",
      "Loss: 0.022166945040225983\n",
      "Loss: 0.027304138988256454\n",
      "Loss: 0.03620032221078873\n",
      "Loss: 0.02183651737868786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m images_batch \u001b[38;5;241m=\u001b[39m images_batch\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     33\u001b[0m text_embeds_batch \u001b[38;5;241m=\u001b[39m text_embeds_batch\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtext_embeds_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet_number\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munet_toTrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     37\u001b[0m trainer\u001b[38;5;241m.\u001b[39mupdate(unet_number \u001b[38;5;241m=\u001b[39m unet_toTrain)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/imagen_pytorch/trainer.py:132\u001b[0m, in \u001b[0;36mcast_torch_tensor.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m args, kwargs_values \u001b[38;5;241m=\u001b[39m all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n\u001b[1;32m    130\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mzip\u001b[39m(kwargs_keys, kwargs_values)))\n\u001b[0;32m--> 132\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/imagen_pytorch/trainer.py:987\u001b[0m, in \u001b[0;36mImagenTrainer.forward\u001b[0;34m(self, unet_number, max_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 987\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/accelerate/accelerator.py:2450\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2450\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Workflow code for less supervised training\n",
    "num_epochs_perRound = 120\n",
    "unet_toTrain = 1\n",
    "\n",
    "rounds = 5\n",
    "change_unet_afterRound = 2\n",
    "\n",
    "trainer = ImagenTrainer(imagen).cuda()\n",
    "\n",
    "rounds_scores = []\n",
    "\n",
    "for round in range(rounds):\n",
    "    if change_unet_afterRound == i:\n",
    "        unet_toTrain = 2\n",
    "    imagen.train()\n",
    "    trainer = ImagenTrainer(imagen).cuda()\n",
    "    trainer.train()\n",
    "    clear_output()\n",
    "    \n",
    "    trainer.load(using_checkpoint)\n",
    "    clear_output()\n",
    "    losses = 0\n",
    "    round_losses = 0\n",
    "    for epoch in range(num_epochs_perRound):\n",
    "        clear_output()\n",
    "        print(f\"Round {round+1}, Epoch {epoch+1}/{num_epochs_perRound}, Training UNet {unet_toTrain}, Prev epoch avg loss: {losses/len(dataloader)}\")\n",
    "        round_losses += losses/len(dataloader)\n",
    "        losses = 0\n",
    "        for images_batch, text_embeds_batch in dataloader:\n",
    "            if images_batch.numel() == 0: continue\n",
    "\n",
    "            images_batch = images_batch.cuda()\n",
    "            text_embeds_batch = text_embeds_batch.cuda()\n",
    "\n",
    "            loss = trainer(images_batch, text_embeds = text_embeds_batch, unet_number = unet_toTrain)\n",
    "            losses += loss\n",
    "            trainer.update(unet_number = unet_toTrain)\n",
    "            print(f\"Loss: {loss}\")\n",
    "        if epoch == num_epochs_perRound - 1:\n",
    "            print(f\"Last epoch avg loss: {losses/len(dataloader)}\")\n",
    "    rounds_scores.append(round_losses/num_epochs_perRound)\n",
    "    trainer.save(using_checkpoint)\n",
    "\n",
    "clear_output()\n",
    "for i, score in enumerate(rounds_scores):\n",
    "    print(f\"Round {i+1} avg loss: {score}\")\n",
    "\n",
    "images = trainer.sample(texts = [\n",
    "    'two zebras in a field',\n",
    "    'two zebras in a zoo',\n",
    "    'the zebra is black and white'\n",
    "], cond_scale = 3.)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "images_cpu = images.cpu().detach()\n",
    "images_cpu = torch.clamp(images_cpu, 0, 1)\n",
    "grid = vutils.make_grid(images_cpu, nrow=len(images_cpu), padding=2, normalize=False)\n",
    "np_grid = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np_grid)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f301926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images\n",
    "\n",
    "images = trainer.sample(texts = [\n",
    "    'two zebras in a field',\n",
    "    'two zebras in a zoo',\n",
    "    'the zebra is black and white'\n",
    "], cond_scale = 3.)\n",
    "\n",
    "# Code for displaying the generated images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Move images tensor to CPU and detach from gradient computation\n",
    "images_cpu = images.cpu().detach()\n",
    "\n",
    "# If your images are normalized to [-1, 1], denormalize them to [0, 1]\n",
    "# images_cpu = (images_cpu + 1) / 2 # Uncomment if you used transforms.Normalize(...)\n",
    "\n",
    "# Clamp values to [0, 1] just in case\n",
    "images_cpu = torch.clamp(images_cpu, 0, 1)\n",
    "\n",
    "# Make a grid of images (rows = number of samples)\n",
    "grid = vutils.make_grid(images_cpu, nrow=len(images_cpu), padding=2, normalize=False) # normalize=False as we clamped\n",
    "\n",
    "# Convert grid tensor to numpy array and transpose dimensions for matplotlib\n",
    "# (C, H, W) -> (H, W, C)\n",
    "np_grid = grid.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Display the grid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(np_grid)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
